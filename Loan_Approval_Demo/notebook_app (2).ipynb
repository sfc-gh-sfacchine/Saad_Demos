{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8386bffc-2180-4c18-ac00-27b17f3db68a",
   "metadata": {
    "name": "Title",
    "collapsed": false
   },
   "source": " ## Snowflake Model Registry\n\n- In this notebook, we will illustrate how to train an XGBoost model with the loan approval dataset using the Snowpark ML Model API.\n- We also show how to do inference and manage models via Model Registry or as a UDF\n\nThe Snowpark ML Model API supports `scikit-learn`, `xgboost`, and `lightgbm` models.\n\n### Import Libraries"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "ImportLibraries",
    "collapsed": false
   },
   "source": "# Import Snowpark \nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.snowpark.functions import udf\nimport snowflake.snowpark.functions as F\n\n# Import Misc Libraries \nimport pandas as pd\nimport altair as alt\nimport numpy as np\nimport streamlit as st\n\n# Import Snowpark ML \nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.registry import registry\nfrom snowflake.ml._internal.utils import identifier\nfrom snowflake.ml.modeling.metrics import f1_score, confusion_matrix\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.preprocessing import MinMaxScaler, OrdinalEncoder\n\n# Setup a Session \nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "027e792e-38ef-47e4-be49-c3ea46216ab5",
   "metadata": {
    "language": "python",
    "name": "CELL2",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.use_warehouse(\"TASTY_DEV_WH\")\nsession.use_database(\"FEATURES\")\nsession.use_schema(\"PUBLIC\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cb4fdcb9-0e57-43a3-bfda-98c64a699d0a",
   "metadata": {
    "name": "Subtitle1",
    "collapsed": false
   },
   "source": "### Load data into a dataframe and show results"
  },
  {
   "cell_type": "code",
   "id": "ab785ebd-406c-45eb-8ed2-000869547d9f",
   "metadata": {
    "language": "python",
    "name": "ShowData",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Load the diamond data into a dataframe\ncredit_df = session.table('FEATURES.PUBLIC.CREDIT_RECORD')\napplication_df = session.table('FEATURES.PUBLIC.APPLICATION_RECORD')\n# Show the data \napplication_df.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8910a322-9b2f-4d4e-bd18-d2990487ede6",
   "metadata": {
    "name": "DataCleaning",
    "collapsed": false
   },
   "source": "### Perform data cleansing, including feature engineering and null value imputation."
  },
  {
   "cell_type": "code",
   "id": "c8f42768-8338-4c54-93d5-6e02e8ad5687",
   "metadata": {
    "language": "python",
    "name": "NullValueImputation",
    "collapsed": false
   },
   "outputs": [],
   "source": "application_df = application_df.with_column('AGE', F.floor(F.abs(F.col('DAYS_BIRTH')) / 365))\napplication_df = application_df.drop(\"DAYS_BIRTH\")\napplication_df = application_df.fillna(application_df[[F.mode('OCCUPATION_TYPE')]].collect()[0][0], subset='OCCUPATION_TYPE')\n\n# Add in additional dataset to get target column, which we will join on ID\ncredit_record_sdf = credit_df.with_column('TARGET', \n                                                  F.when((F.col('STATUS') == '2') | \n                                                         (F.col('STATUS') == '3') | \n                                                         (F.col('STATUS') == '4') | \n                                                         (F.col('STATUS') == '5'), 'YES'))\ncpunt = credit_record_sdf.group_by('ID').agg(F.count('TARGET').as_('TARGET'))\ncpunt = cpunt.with_column('TARGET', F.when(F.col('TARGET') > 0, 1).otherwise(0))\ncpunt = cpunt.drop('DEP_VALUE')\n\n# Joining our target variable to our customer records\napplication_record_sdf = application_df.join(cpunt, using_columns='ID', join_type='inner')\n# Finally we can drop the ID variable as we won't use it for training\napplication_record_sdf = application_record_sdf.drop('ID')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4636b7c9-606a-46e8-86ba-34aa90baa5b7",
   "metadata": {
    "language": "python",
    "name": "FullTable",
    "collapsed": false
   },
   "outputs": [],
   "source": "application_record_sdf.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b71d494-be30-4bc3-a34a-8414c1138a90",
   "metadata": {
    "language": "python",
    "name": "Categorize",
    "collapsed": false
   },
   "outputs": [],
   "source": "CATEGORICAL_COLUMNS = [\"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\", \"NAME_INCOME_TYPE\", \"NAME_EDUCATION_TYPE\", \"NAME_FAMILY_STATUS\", \"NAME_HOUSING_TYPE\",\"OCCUPATION_TYPE\"]\nCATEGORICAL_COLUMNS_OE = [i+\"_OUT\" for i in CATEGORICAL_COLUMNS]\nNUMERICAL_COLUMNS = [\"CNT_CHILDREN\", \"AMT_INCOME_TOTAL\", \"DAYS_EMPLOYED\", \"FLAG_MOBIL\", \"FLAG_WORK_PHONE\", \"FLAG_PHONE\", \"FLAG_EMAIL\", \"CNT_FAM_MEMBERS\", \"AGE\"]\nLABEL_COLUMNS = [\"TARGET\"]\nOUTPUT_COLUMNS = [\"PREDICTED_CAT\"]",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96e3d2c1-fcf2-4f48-8db2-a21154742d04",
   "metadata": {
    "name": "Subtitle2",
    "collapsed": false
   },
   "source": "### Build a simple XGBoost Regression model\n\nWhat's happening here? \n\n- The `model.fit()` function actually creates a temporary stored procedure in the background. This also means that the model training is a single-node operation. Be sure to use a [Snowpark Optimized Warehouse](https://docs.snowflake.com/en/user-guide/warehouses-snowpark-optimized) if you need more memory. We are just using an XS Standard Virtual Warehouse here, which we created at the beginning of this quickstart.\n- The `model.predict()` function actualls creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data.\n\nYou can check the query history once you execute the following cell to check."
  },
  {
   "cell_type": "code",
   "id": "ea624a56-1757-43fd-b4ca-2ec04b5138c9",
   "metadata": {
    "language": "python",
    "name": "TrainAndAnalyze",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Define the categories \n\n# Initialize a pipeline of transforms\npipeline = Pipeline(\n    steps=[\n        (\n            \"OE\",\n            OrdinalEncoder(\n            input_cols=CATEGORICAL_COLUMNS,\n            output_cols=CATEGORICAL_COLUMNS_OE\n            )\n        ),\n        (\n            \"MMS\",\n            MinMaxScaler(\n            clip=True,\n            input_cols=NUMERICAL_COLUMNS,\n            output_cols=NUMERICAL_COLUMNS,\n            )\n        ),\n        (\n            \"classifier\",\n            XGBClassifier(\n            input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n            label_cols=LABEL_COLUMNS,\n            output_cols=OUTPUT_COLUMNS\n            )\n        )\n    ]\n)\n\n# Split the data into train and test sets\ntrain_df, test_df = application_record_sdf.random_split(weights=[0.9, 0.1], seed=0)\n\n# Train on the train set \npipeline.fit(train_df)\n\n# Predict on the test set \npredicted_results = pipeline.predict(test_df)\n\n# Analyze the results using Snowpark ML's MAPE.\nf1 = f1_score(df=predicted_results, y_true_col_names=\"TARGET\", \n                                        y_pred_col_names=\"PREDICTED_CAT\")\n\npredicted_results.select(\"TARGET\", \"PREDICTED_CAT\").show()\nprint(f\"F1 Score: {f1}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4eead64d-d5bc-4cc3-8d49-80c1fae28566",
   "metadata": {
    "name": "CELL0",
    "collapsed": false
   },
   "source": "### Looking at confusion matrix to investigate precision and recall"
  },
  {
   "cell_type": "code",
   "id": "109009a2-1c94-4171-af5c-2c4887ab57c2",
   "metadata": {
    "language": "python",
    "name": "ConfusionMatrix",
    "collapsed": false
   },
   "outputs": [],
   "source": "conf_matrix_df = confusion_matrix(df = predicted_results, y_true_col_name = 'TARGET', y_pred_col_name = 'PREDICTED_CAT')\nconf_matrix_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f28dfbc-2817-4b73-ac9a-09cda4072510",
   "metadata": {
    "name": "Subtitle3",
    "collapsed": false
   },
   "source": "## Model deployment using Model Registry\n\nNow, with Snowpark ML's model registry, we have a Snowflake native model versioning and deployment framework. This allows us to log models, tag parameters and metrics, track metadata, create versions, and ultimately deploy models into a Snowflake warehouse or Snowpark Container Service for batch scoring tasks.\n\n### Open Model Registry and see all Models"
  },
  {
   "cell_type": "code",
   "id": "7e285375-226d-472d-bf9d-4e2be3b0cca9",
   "metadata": {
    "language": "python",
    "name": "InspectRegistry",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Get the current database and schema from the current session \ncurrent_db = identifier._get_unescaped_name(session.get_current_database())\ncurrent_schema = identifier._get_unescaped_name(session.get_current_schema())\n\n# Open the model registry and point ot the appropriate database and schema \nnative_registry = registry.Registry(session=session, database_name=current_db, schema_name=current_schema)\n\n# Show the models in our registry \nnative_registry.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76b52f53-b8bd-42b6-8587-7e47b85462e3",
   "metadata": {
    "name": "Subtitle4",
    "collapsed": false
   },
   "source": "### Log a Model"
  },
  {
   "cell_type": "code",
   "id": "c041a9be-279b-4394-8119-e4194a545d79",
   "metadata": {
    "language": "python",
    "name": "LogModel",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Get sample input data to pass into the registry logging function\nsample_data = train_df.select(CATEGORICAL_COLUMNS+NUMERICAL_COLUMNS).limit(100)\n\n# Define model name and version\nmodel_name = \"loan_approval_model\"\nmodel_version = 'v6'\n\n# Log the model \nmodel_ver = native_registry.log_model( \n    model_name=model_name,\n    version_name=model_version,\n    model=pipeline,\n    sample_input_data=sample_data,\n    options={\"embed_local_ml_library\": True, # This option is enabled to pull latest dev code changes.\n             \"relax\": True}) # relax dependencies\n\n\n# Add evaluation metric\nmodel_ver.set_metric(metric_name=\"f1\", value=f1)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "06ebc195-8184-48d0-b57b-4e73a084a301",
   "metadata": {
    "name": "Subtitle5",
    "collapsed": false
   },
   "source": "### Inspect the Model Registry again "
  },
  {
   "cell_type": "code",
   "id": "d1407a92-ed63-4fbb-9329-6fd5d7d1ac42",
   "metadata": {
    "language": "python",
    "name": "InspectRegisteryAgain",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Show the models in our registry \nnative_registry.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d5f42bf-055f-4947-a616-41d278793844",
   "metadata": {
    "name": "Subtitle6",
    "collapsed": false
   },
   "source": "### See the versions of a registered Model"
  },
  {
   "cell_type": "code",
   "id": "1ba55976-c90b-4e05-b3d9-e9ad02b24227",
   "metadata": {
    "language": "python",
    "name": "SeeModelVersions",
    "collapsed": false
   },
   "outputs": [],
   "source": "\n# Show the versions of a registered model \nm = native_registry.get_model('LOAN_APPROVAL_MODEL')\nm.show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "afe37679-c0ba-4434-a4d2-e131f68dcccd",
   "metadata": {
    "name": "Subtitle7",
    "collapsed": false
   },
   "source": "### Add or change comments for the model"
  },
  {
   "cell_type": "code",
   "id": "492933b0-71cc-43e5-9e84-b3ae6f83e259",
   "metadata": {
    "language": "python",
    "name": "ChangeComments",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Add a comment\nm.comment = \"Adding in more categorical columns - 2/7/24\"\n\n# Show the versions again \nprint(m.comment)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3bc74c88-5477-4279-bde8-0e572d7e0467",
   "metadata": {
    "name": "ChangeDefaultVersion",
    "collapsed": false
   },
   "source": "### Set the default version for the model\n"
  },
  {
   "cell_type": "code",
   "id": "ad48ed1a-6153-4fae-89fa-6d3e6f6fcf4b",
   "metadata": {
    "language": "python",
    "name": "ShowFunctions",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Set the default version for the model \nm.default = \"V4\"\n\n# Show the versions \nm.show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4658cad0-eb98-429c-a6bd-ac507e969560",
   "metadata": {
    "name": "Subtilte8",
    "collapsed": false
   },
   "source": "### Use a model from the registry"
  },
  {
   "cell_type": "code",
   "id": "c1427a82-54f5-43f4-af0d-f75366d8eff7",
   "metadata": {
    "language": "python",
    "name": "UseModel",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Get the version of the model we want to work with \nmodel_ver = native_registry.get_model('LOAN_APPROVAL_MODEL').version('V4')\n\n# Run the model passing in the test dataframe and using the predict function \nremote_prediction = model_ver.run(test_df, function_name=\"predict\")\n\n# Convert the output to pandas \nremote_prediction.to_pandas()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21e9bd1e-3ceb-41e6-846a-d743bf91851e",
   "metadata": {
    "language": "python",
    "name": "LargeDataset",
    "collapsed": false
   },
   "outputs": [],
   "source": "large_application_df = session.table(\"FEATURES.PUBLIC.APPLICATION_RECORD_LARGE\")\nlarge_application_df.count()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "75273ebb-1bab-477f-8cc8-984acc9d26a0",
   "metadata": {
    "name": "CELL1",
    "collapsed": false
   },
   "source": "## Inference is quick!\n\nSnowflake parallelizes the batch inference job so that you can get millions of predictions using the tiniest warehouse available (XSMALL). The below runs the model we created on 10 million rows. "
  },
  {
   "cell_type": "code",
   "id": "09afd995-f323-4b48-a94b-060acf6fa451",
   "metadata": {
    "language": "python",
    "name": "BatchPrediction",
    "collapsed": false
   },
   "outputs": [],
   "source": "import time\nstart_time = time.time()\nremote_prediction = model_ver.run(large_application_df, function_name=\"predict\")\nend_time = time.time()\ntime_taken = end_time - start_time\nprint(\"Time taken to run a batch prediction on 10 Million records using an XSMALL warehouse: {:.2f} seconds\".format(time_taken))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1c09cd33-c4c5-4dd8-88c4-08ce435e2496",
   "metadata": {
    "language": "python",
    "name": "CELL12",
    "collapsed": false
   },
   "outputs": [],
   "source": "remote_prediction.show()",
   "execution_count": null
  }
 ]
}